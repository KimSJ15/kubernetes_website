<!Doctype html>
<html id="docs">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href='https://fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="/css/styles.css"/>
	<script src="/js/script.js"></script>
    <script src="/js/jquery-2.2.0.min.js"></script>
    <script src="/js/non-mini.js"></script>
    <title>Kubernetes - Replication Controller</title>
</head>
<body>
<div id="cellophane" onclick="kub.toggleMenu()"></div>
<header>
	<a href="/" class="logo"></a>
	<div class="nav-buttons" data-auto-burger="primary">
		<a href="/docs" class="button" id="viewDocs">View Documentation</a>
		<a href="/get-started" class="button" id="tryKubernetes">Try Kubernetes</a>
		<button id="hamburger" onclick="kub.toggleMenu()" data-auto-burger-exclude><div></div></button>
	</div>

	<nav id="mainNav">
		<main data-auto-burger="primary">
			<div class="nav-box">
				<h3><a href="">Get Started</a></h3>
				<p>Built for a multi-cloud world, public, private or hybrid. Seamlessly roll out new features.</p>
			</div>
			<div class="nav-box">
				<h3><a href="">Documentation</a></h3>
				<p>Pellentesque in ipsum id orci porta dapibus. Nulla porttitor accumsan tincidunt. </p>
			</div>
			<div class="nav-box">
				<h3><a href="">Community</a></h3>
				<p>Vestibulum ac diam sit amet quam vehicula elementum sed sit amet dui. </p>
			</div>
			<div class="nav-box">
				<h3><a href="">Blog</a></h3>
				<p>Curabitur arcu erat, accumsan id imperdiet et, porttitor at sem. Quisque velit nisi, pretium ut lacinia in. </p>
			</div>
		</main>
		<main data-auto-burger="primary">
			<div class="left">
				<h5 class="github-invite">Interested in hacking on the core Kubernetes code base?</h5>
				<a href="" class="button">View On Github</a>
			</div>

			<div class="right">
				<h5 class="github-invite">Explore the community</h5>
				<div class="social">
					<a href="https://twitter.com/kubernetesio" class="Twitter"><span>twitter</span></a>
					<a href="https://github.com/kubernetes/kubernetes" class="github"><span>Github</span></a>
					<a href="http://slack.k8s.io/" class="slack"><span>Slack</span></a>
					<a href="http://stackoverflow.com/questions/tagged/kubernetes" class="stack-overflow"><span>stackoverflow</span></a>
					<a href="https://groups.google.com/forum/#!forum/google-containers" class="mailing-list"><span>Mailing List</span></a>
				</div>
			</div>
			<div class="clear" style="clear: both"></div>
		</main>
	</nav>
</header>

<!--  HERO  -->
<section id="hero" class="light-text">
	<h1></h1>
	<h5></h5>
	<div id="vendorStrip" class="light-text">
		<ul>
			<li><a href="/v1.1/guides">GUIDES</a></li>
			<li><a href="/v1.1/reference">REFERENCE</a></li>
			<li><a href="/v1.1/samples">SAMPLES</a></li>
			<li><a href="/v1.1/support">SUPPORT</a></li>
		</ul>
		<div class="dropdown">
			<div class="readout"></div>
			<a href="/v1.1">Version 1.1</a>
			<a href="/v1.0">Version 1.0</a>
		</div>
		<input type="text" id="search" placeholder="Search the docs">
	</div>
</section>

<section id="encyclopedia">
	<div id="docsToc">
        <div class="pi-accordion">
            
            

        </div> <!-- /pi-accordion -->
	</div> <!-- /docsToc -->
	<div id="docsContent">
    	<h1>Replication Controller</h1>
		<!-- BEGIN MUNGE: UNVERSIONED_WARNING -->

<!-- END MUNGE: UNVERSIONED_WARNING -->

<h1 id="replication-controller">Replication Controller</h1>

<p><strong>Table of Contents</strong>
<!-- BEGIN MUNGE: GENERATED_TOC --></p>

<ul>
  <li><a href="#replication-controller">Replication Controller</a>
    <ul>
      <li><a href="#what-is-a-replication-controller">What is a <em>replication controller</em>?</a></li>
      <li><a href="#how-does-a-replication-controller-work">How does a replication controller work?</a>
        <ul>
          <li><a href="#pod-template">Pod template</a></li>
          <li><a href="#labels">Labels</a></li>
        </ul>
      </li>
      <li><a href="#responsibilities-of-the-replication-controller">Responsibilities of the replication controller</a></li>
      <li><a href="#common-usage-patterns">Common usage patterns</a>
        <ul>
          <li><a href="#rescheduling">Rescheduling</a></li>
          <li><a href="#scaling">Scaling</a></li>
          <li><a href="#rolling-updates">Rolling updates</a></li>
          <li><a href="#multiple-release-tracks">Multiple release tracks</a></li>
        </ul>
      </li>
      <li><a href="#api-object">API Object</a></li>
    </ul>
  </li>
</ul>

<!-- END MUNGE: GENERATED_TOC -->

<h2 id="what-is-a-replication-controller">What is a <em>replication controller</em>?</h2>

<p>A <em>replication controller</em> ensures that a specified number of pod “replicas” are running at any one time.  If there are too many, it will kill some.  If there are too few, it will start more. Unlike in the case where a user directly created pods, a replication controller replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a replication controller even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A replication controller delegates local container restarts to some agent on the node (e.g., Kubelet or Docker).</p>

<p>As discussed in <a href="pod-states.html">life of a pod</a>, <code>ReplicationController</code> is <em>only</em> appropriate for pods with <code>RestartPolicy = Always</code>. (Note: If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>.)  <code>ReplicationController</code> should refuse to instantiate any pod that has a different restart policy. As discussed in <a href="http://issue.k8s.io/503#issuecomment-50169443">issue #503</a>, we expect other types of controllers to be added to Kubernetes to handle other types of workloads, such as build/test and batch workloads, in the future.</p>

<p>A replication controller will never terminate on its own, but it isn’t expected to be as long-lived as services. Services may be composed of pods controlled by multiple replication controllers, and it is expected that many replication controllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the replication controllers that maintain the pods of the services.</p>

<h2 id="how-does-a-replication-controller-work">How does a replication controller work?</h2>

<h3 id="pod-template">Pod template</h3>

<p>A replication controller creates new pods from a template, which is currently inline in the <code>ReplicationController</code> object, but which we plan to extract into its own resource <a href="http://issue.k8s.io/170">#170</a>.</p>

<p>Rather than specifying the current desired state of all replicas, pod templates are like cookie cutters. Once a cookie has been cut, the cookie has no relationship to the cutter. There is no quantum entanglement. Subsequent changes to the template or even switching to a new template has no direct effect on the pods already created. Similarly, pods created by a replication controller may subsequently be updated directly. This is in deliberate contrast to pods, which do specify the current desired state of all containers belonging to the pod. This approach radically simplifies system semantics and increases the flexibility of the primitive, as demonstrated by the use cases explained below.</p>

<p>Pods created by a replication controller are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but replication controllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href="https://coreos.com/docs/distributed-configuration/etcd-modules/">etcd lock module</a> or <a href="https://www.rabbitmq.com/tutorials/tutorial-two-python.html">RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (e.g., cpu or memory), should be performed by another online controller process, not unlike the replication controller itself.</p>

<h3 id="labels">Labels</h3>

<p>The population of pods that a replication controller is monitoring is defined with a <a href="labels.html#label-selectors">label selector</a>, which creates a loosely coupled relationship between the controller and the pods controlled, in contrast to pods, which are more tightly coupled to their definition. We deliberately chose not to represent the set of pods controlled using a fixed-length array of pod specifications, because our experience is that approach increases complexity of management operations, for both clients and the system.</p>

<p>The replication controller should verify that the pods created from the specified template have labels that match its label selector. Though it isn’t verified yet, you should also ensure that only one replication controller controls any given pod, by ensuring that the label selectors of replication controllers do not target overlapping sets. If you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself with –cascade=false until there are no controllers with an overlapping superset of selectors.</p>

<p>Note that replication controllers may themselves have labels and would generally carry the labels their corresponding pods have in common, but these labels do not affect the behavior of the replication controllers.</p>

<p>Pods may be removed from a replication controller’s target set by changing their labels. This technique may be used to remove pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p>

<p>Similarly, deleting a replication controller using the API does not affect the pods it created. Its <code>replicas</code> field must first be set to <code>0</code> in order to delete the pods controlled. (Note that the client tool, <code>kubectl</code>, provides a single operation, <a href="kubectl/kubectl_delete.html">delete</a> to delete both the replication controller and the pods it controls. If you want to leave the pods running when deleting a replication controller, specify <code>--cascade=false</code>. However, there is no such operation in the API at the moment)</p>

<h2 id="responsibilities-of-the-replication-controller">Responsibilities of the replication controller</h2>

<p>The replication controller simply ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href="http://issue.k8s.io/620">readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p>

<p>The replication controller is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href="http://issue.k8s.io/492">#492</a>), which would change its <code>replicas</code> field. We will not add scheduling policies (e.g., <a href="http://issue.k8s.io/367#issuecomment-48428019">spreading</a>) to the replication controller. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href="http://issue.k8s.io/170">#170</a>).</p>

<p>The replication controller is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The “macro” operations currently supported by kubectl (run, stop, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like [Spinnaker] (http://spinnaker.io/) managing replication controllers, auto-scalers, services, scheduling policies, canaries, etc.</p>

<h2 id="common-usage-patterns">Common usage patterns</h2>

<h3 id="rescheduling">Rescheduling</h3>

<p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a replication controller will ensure that the specified number of pods exists, even in the event of node failure or pod termination (e.g., due to an action by another control agent).</p>

<h3 id="scaling">Scaling</h3>

<p>The replication controller makes it easy to scale the number of replicas up or down, either manually or by an auto-scaling control agent, by simply updating the <code>replicas</code> field.</p>

<h3 id="rolling-updates">Rolling updates</h3>

<p>The replication controller is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p>

<p>As explained in <a href="http://issue.k8s.io/1353">#1353</a>, the recommended approach is to create a new replication controller with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p>

<p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p>

<p>The two replication controllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p>

<p>Rolling update is implemented in the client tool
<a href="kubectl/kubectl_rolling-update.html">kubectl</a></p>

<h3 id="multiple-release-tracks">Multiple release tracks</h3>

<p>In addition to running multiple releases of an application while a rolling update is in progress, it’s common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p>

<p>For instance, a service might target all pods with <code>tier in (frontend), environment in (prod)</code>.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to ‘canary’ a new version of this component.  You could set up a replication controller with <code>replicas</code> set to 9 for the bulk of the replicas, with labels <code>tier=frontend, environment=prod, track=stable</code>, and another replication controller with <code>replicas</code> set to 1 for the canary, with labels <code>tier=frontend, environment=prod, track=canary</code>.  Now the service is covering both the canary and non-canary pods.  But you can mess with the replication controllers separately to test things out, monitor the results, etc.</p>

<h2 id="api-object">API Object</h2>

<p>Replication controller is a top-level resource in the kubernetes REST API. More details about the
API object can be found at: <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_replicationcontroller">ReplicationController API
object</a>.</p>

<!-- BEGIN MUNGE: IS_VERSIONED -->
<!-- TAG IS_VERSIONED -->
<!-- END MUNGE: IS_VERSIONED -->

<!-- BEGIN MUNGE: GENERATED_ANALYTICS -->
<p><a href=""><img src="https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/docs/user-guide/replication-controller.md?pixel" alt="Analytics" /></a>
<!-- END MUNGE: GENERATED_ANALYTICS --></p>


	</div>
</section>


<footer>
	<main class="light-text">
		<nav>
			<a href="/getting-started.html">Getting Started</a>
			<a href="/docs.html">Documentation</a>
			<a href="http://blog.kubernetes.io/">Blog</a>
			<a href="/foobang.html">Community</a>
		</nav>
		<div class="social">
			<a href="https://twitter.com/kubernetesio" class="twitter"><span>twitter</span></a>
			<a href="https://github.com/kubernetes/kubernetes" class="github"><span>Github</span></a>
			<a href="http://slack.k8s.io/" class="slack"><span>Slack</span></a>
			<a href="http://stackoverflow.com/questions/tagged/kubernetes" class="stack-overflow"><span>stackoverflow</span></a>
			<a href="https://groups.google.com/forum/#!forum/google-containers" class="mailing-list"><span>Mailing List</span></a>
			<label for="wishField">I wish this page <input type="text" id="wishField" name="wishField" placeholder="made better textfield suggestions"></label>
		</div>
		<div class="center">&copy; 2016 Kubernetes</div>
	</main>
</footer>

</body>
</html>



